<!DOCTYPE html>
<html lang="en">
<head>


    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Linear Regression</title>
    <link rel="stylesheet" href="posts.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>


<body>
    <a class="link" href="index.html">
        
    </a>
    <div class="container-posts">
        <h1 class="title">
            Linear Regression
        </h1>
        <br>
        <br>
        <h2>
            1. Data
        </h2>
        <p class="paragraph-1">
            $$

            \mathbf{X} = 
            \begin{bmatrix}
            x_{1,1} & x_{1,2} & \cdots & x_{1,d} \\
            x_{2,1} & x_{2,2} & \cdots & x_{2,d} \\
            \vdots & \vdots & \ddots & \vdots \\
            x_{n,1} & x_{n,2} & \cdots & x_{n,d} 
            \end{bmatrix}

            \quad

            \mathbf{Y} = 
            \begin{bmatrix}
            y_1 \\
            y_2 \\
            \vdots & \\
            y_n
            \end{bmatrix}

            $$  
        </p>
        <p class="paragraph-1">
            <b>Notice</b> : <b>n</b> is the number of data points and <b>d</b> is the number of dimensions
        </p>
        <br><br>
        <h2>
            2. Function for prediction
        </h2>
        <p class="paragraph-1">
            <b>Function for a data point :</b>  
        </p>
        <p class="paragraph-1">
            $$ f(x) = \hat y \approx y $$
            $$ \, $$
            $$ \hat y = x_1w_1 + x_2w_2 + x_3w_3\: + \:...\: + \:x_dw_d + w_0 $$
        </p>
        <br><br>
        <h2>
            3. Mathematics
        </h2>
        <p class="paragraph-1">
            Vector <b>x</b> for a data point : $$ \mathbf{x} = [1,\: x_1,\: x_2,\: x_3,\: ...,\: x_d] $$
        </p>
        <p class="paragraph-1">
            Vector <b>w</b> : $$ \mathbf{w} = [w_0,\: w_1,\: w_2,\: w_3,\: ...,\: w_d]^T $$
        </p>
        <p class="paragraph-1">
            So : $$ \hat y = \mathbf{x}\mathbf{w} $$
        </p>
        <p class="paragraph-1">
            <b>Loss Function for a data point : </b>
            $$ L(\mathbf{w}) = \hat y - y $$
            $$\qquad \quad \:\:\:\: = (\hat y - y)^2 $$
            $$\qquad \qquad \quad = (\mathbf{x}\mathbf{w} - y)^2 $$
        </p>
        <br>
        <h2>
            3.1 Ordinary Least Squares Regression
        </h2>
        <p class="paragraph-1">
            <b>Loss Function for all data points :   </b>  
            $$ L(\mathbf{w}) = \dfrac{1}{2} \sum_{i=1}^{N} (\mathbf{x_i}\mathbf{w} - y_i)^2 $$
            $$\quad = \dfrac{1}{2} \| \mathbf{X}\mathbf{w} - \mathbf{y} \|_2^2 $$
            So :
            $$ \mathbf{w} = \arg\min_{\mathbf{w}} L(\mathbf{w}) $$
        </p>
        <br>
        <p class="paragraph-1">
            <b>Solving the gradient of Loss Function : </b><br>
            $$ \dfrac{\partial L(\mathbf{w})}{\partial \mathbf{w}} = \mathbf{X}^T(\mathbf{X} \mathbf{w} - \mathbf{y}) $$
            <b>Solving parameters by the zero gradient solution :</b>
            $$ \:\:\: \mathbf{X}^T(\mathbf{X} \mathbf{w} - \mathbf{y}) = 0 $$
            $$ \mathbf{X}^T \mathbf{X} \mathbf{w} - \mathbf{X}^T \mathbf{y} = 0 $$
            $$ \qquad \qquad \:\:\:\: \mathbf{X}^T \mathbf{X} \mathbf{w} = \mathbf{X}^T \mathbf{y} $$
            $$ \qquad \qquad \:\:\:\: (\mathbf{X}^T \mathbf{X})^{-1} \: \mathbf{X}^T \mathbf{X} \mathbf{w} = (\mathbf{X}^T \mathbf{X})^{-1} \: \mathbf{X}^T \mathbf{y} $$
            $$ \qquad \qquad \qquad \qquad \qquad \:\:\:\:\:\:\: \mathbf{w} = (\mathbf{X}^T \mathbf{X})^{-1} \: \mathbf{X}^T \mathbf{y}$$
        </p>
        <br>
        <h2>
            3.2 Ridge Regression (L2 Regularization)
        </h2>
        <p class="paragraph-1">
            <b>Loss Function for all data points :   </b>  
            $$ L(\mathbf{w}) = \sum_{i=1}^{n} (\mathbf{x_i}\mathbf{w} - y_i)^2 + \alpha \sum_{j=0}^{d} w_i^2 $$
            $$\:\: = \| \mathbf{X}\mathbf{w} - \mathbf{y} \|_2^2 + \alpha \| \mathbf{w} \|_2^2  $$
            So :
            $$ \mathbf{w} = \arg\min_{\mathbf{w}} L(\mathbf{w}) $$
        </p>
        <br>
        <p class="paragraph-1">
            <b>Solving the gradient of Loss Function : </b><br>
            $$ \dfrac{\partial L(\mathbf{w})}{\partial \mathbf{w}} = 2 \mathbf{X}^T(\mathbf{X} \mathbf{w} - \mathbf{y}) + 2 \alpha \mathbf{w} $$
            <b>Solving parameters by the zero gradient solution :</b>
            $$ 2 \mathbf{X}^T(\mathbf{X} \mathbf{w} - \mathbf{y}) + 2 \alpha \mathbf{w} = 0 $$
            $$ \quad \mathbf{X}^T(\mathbf{X} \mathbf{w} - \mathbf{y}) + \alpha \mathbf{w} = 0 $$
            $$ \:\: \mathbf{X}^T \mathbf{X} \mathbf{w} - \mathbf{X}^T \mathbf{y} + \alpha \mathbf{w} = 0$$
            $$ \qquad \qquad \quad \:\:  \mathbf{X}^T \mathbf{X} \mathbf{w} + \alpha \mathbf{w} = \mathbf{X}^T \mathbf{y} $$
            $$ \qquad \qquad \quad (\mathbf{X}^T \mathbf{X} + \alpha I) \mathbf{w} = \mathbf{X}^T \mathbf{y} $$ 
            $$ \qquad \qquad \quad (\mathbf{X}^T \mathbf{X} + \alpha I)^{-1} (\mathbf{X}^T \mathbf{X} + \alpha I) \mathbf{w} = (\mathbf{X}^T \mathbf{X} + \alpha I)^{-1} \mathbf{X}^T \mathbf{y} $$ 
            $$ \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad \mathbf{w} = (\mathbf{X}^T \mathbf{X} + \alpha I)^{-1} \mathbf{X}^T \mathbf{y} $$
        </p>

        <br>
        <h2>
            3.3 Lasso Regression (L1 Regularization)
        </h2>
        <p class="paragraph-1">
            <b>Loss Function for all data points :   </b>  
            $$ L(\mathbf{w}) = \sum_{i=1}^{n} (\mathbf{x_i}\mathbf{w} - y_i)^2 + \alpha \sum_{j=0}^{d} |w_i| $$
            $$\:\: = \| \mathbf{X}\mathbf{w} - \mathbf{y} \|_2^2 + \alpha \| \mathbf{w} \|_1  $$
            So :
            $$ \mathbf{w} = \arg\min_{\mathbf{w}} L(\mathbf{w}) $$
            <b>Solving parameters :</b> It can't be solved because the derivative of the absolute is undefined at zero and we can't ignore the point equals zero, it's important.
        </p>

        <br>
        <h2>
            3.4 ElasticNet Regression
        </h2>
        <p class="paragraph-1">
            <b>Loss Function for all data points :   </b>  
            $$ L(\mathbf{w}) = \sum_{i=1}^{n} (\mathbf{x_i}\mathbf{w} - y_i)^2 + \alpha_1 \sum_{j=0}^{d} |w_i|  + \alpha_2 \sum_{j=0}^{d} w_i^2  $$
            $$\:\: = \| \mathbf{X}\mathbf{w} - \mathbf{y} \|_2^2 + \alpha_1 \| \mathbf{w} \|_1 + \alpha_2 \| \mathbf{w} \|_2  $$
            So :
            $$ \mathbf{w} = \arg\min_{\mathbf{w}} L(\mathbf{w}) $$
            <b>Solving parameters :</b> It can't be solved again.
        </p>

        <br>
        <br>
        <br>
        <br>
        <h2>
            4. Data - Model - Prediction
        </h2>
        <p class="paragraph-1">
            <b>Data : </b>
            $$data \quad \mathbf{X}(n \times d) \quad and \quad target \quad \mathbf{y}(n \times 1)  $$
            <br>
            <b>Model: Choosing model and training :</b>
            output after training: $$ \mathbf{w}(d \times 1) $$
            <br>
            <b>Prediction :</b>
            $$ \hat y = x_1w_1 + x_2w_2 + x_3w_3\: + \:...\: + \:x_dw_d + w_0 $$
        </p>

        <br>
        <h2>
            5. Pseudo inverse matrix
        </h2>
        <h3>
            Pseudo inverse is solution when the inverse matrix can't be calculated
        </h3>
    </div>


</body>
</html>
